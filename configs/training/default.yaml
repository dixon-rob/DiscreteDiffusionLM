# @package _global_
# Default training configuration

defaults:
  - /model/base
  - _self_

# Optimizer
learning_rate: 5e-5
weight_decay: 0.01
betas: [0.9, 0.999]
gradient_clip: 1.0

# Training schedule
num_epochs: 5
batch_size: 4

# Mixed precision training ("no", "fp16", or "bf16")
# bf16: RTX 3090/4090 and newer (Ampere/Ada architectures) - recommended
# fp16: RTX 2080/2090 (Turing) - use this for older RTX cards
# no: Disable mixed precision
mixed_precision: "bf16"

# Gradient checkpointing: saves memory by recomputing activations during backward pass
# Reduces memory usage but adds compute overhead
gradient_checkpointing: true

# Gradient accumulation: accumulate gradients over N steps before optimizer update
# Effective batch size = batch_size * gradient_accumulation_steps
# Useful for simulating larger batch sizes when memory-constrained
gradient_accumulation_steps: 8

# 8-bit Adam optimizer: reduces optimizer memory
# Requires: bitsandbytes
use_8bit_optimizer: true

# Noise schedule
sigma_min: 1e-4
sigma_max: 20.0

# Data
dataset_name: "roneneldan/TinyStories"
train_samples: null  # Use full dataset (~2.1M stories)
val_samples: 20000
context_length: 256
num_workers: 2

# Checkpointing
checkpoint_dir: "./checkpoints"
save_every: 1  # Save every N epochs
save_every_steps: 500  # Save every N steps (0 = disabled)
save_hf_format: true

# Sampling epsilon (avoid t=0 or t=1)
sampling_eps: 1e-3
