# Default training configuration

# Optimizer
learning_rate: 5e-5
weight_decay: 0.01
betas: [0.9, 0.999]
gradient_clip: 1.0

# Training schedule
num_epochs: 30
batch_size: 4

# Mixed precision training ("no", "fp16", or "bf16")
# bf16: RTX 3090/4090 and newer (Ampere/Ada architectures) - recommended
# fp16: RTX 2080/2090 (Turing) - use this for older RTX cards
# no: Disable mixed precision
mixed_precision: "bf16"

# Gradient checkpointing: saves memory by recomputing activations during backward pass
# Reduces memory usage but adds compute overhead
gradient_checkpointing: true

# Gradient accumulation: accumulate gradients over N steps before optimizer update
# Effective batch size = batch_size * gradient_accumulation_steps
# Useful for simulating larger batch sizes when memory-constrained
gradient_accumulation_steps: 8

# Noise schedule
sigma_min: 1e-4
sigma_max: 20.0

# Data
dataset_name: "roneneldan/TinyStories"
train_samples: 50000
val_samples: 2000
context_length: 256
num_workers: 2

# Checkpointing
checkpoint_dir: "./checkpoints"
save_every: 5
save_hf_format: true

# Sampling epsilon (avoid t=0 or t=1)
sampling_eps: 1e-3
