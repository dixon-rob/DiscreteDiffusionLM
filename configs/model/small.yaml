# @package model
# Small model configuration for ~1 hour training
# ~3M parameters, optimized for learning spelling/grammar patterns

block_size: 256         # Maximum sequence length
n_layer: 6              # Number of transformer blocks
n_head: 4               # Number of attention heads
n_embd: 256             # Embedding dimension
cond_dim: 64            # Conditioning (timestep embedding) dimension
dropout: 0.1            # Dropout probability
bias: true              # Whether to use bias in linear layers
